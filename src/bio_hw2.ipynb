{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bio_hw2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUdVlINoydEp"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXCH1qpmyoI8",
        "outputId": "fdc1f34a-6cce-472d-e925-53ad549f2c51"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RQXiQaIQV2H",
        "outputId": "12a00e06-ada3-4402-d11d-5d59e8dea619"
      },
      "source": [
        "!apt-get install bedtools"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  bedtools\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 577 kB of archives.\n",
            "After this operation, 2,040 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 bedtools amd64 2.26.0+dfsg-5 [577 kB]\n",
            "Fetched 577 kB in 2s (333 kB/s)\n",
            "Selecting previously unselected package bedtools.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../bedtools_2.26.0+dfsg-5_amd64.deb ...\n",
            "Unpacking bedtools (2.26.0+dfsg-5) ...\n",
            "Setting up bedtools (2.26.0+dfsg-5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhcZfEDPzabi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce65ab05-3924-43f0-9c81-780f775cdb46"
      },
      "source": [
        "! wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/twoBitToFa\n",
        "! chmod ugo+rwx twoBitToFa\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-02 19:28:20--  http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/twoBitToFa\n",
            "Resolving hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)... 128.114.119.163\n",
            "Connecting to hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)|128.114.119.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9367560 (8.9M)\n",
            "Saving to: ‘twoBitToFa.1’\n",
            "\n",
            "twoBitToFa.1        100%[===================>]   8.93M  5.99MB/s    in 1.5s    \n",
            "\n",
            "2021-12-02 19:28:22 (5.99 MB/s) - ‘twoBitToFa.1’ saved [9367560/9367560]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHY53L9FVxKF"
      },
      "source": [
        "H3F3A_DOHH2_intersect_with_DeepZ = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/H3F3A_DOHH2.intersect_with_DeepZ.bed', sep='\\t', names=['chr', 'start', 'end'])\n",
        "H3F3A_DOHH2_intersect_with_DeepZ['len'] = H3F3A_DOHH2_intersect_with_DeepZ['end'] - H3F3A_DOHH2_intersect_with_DeepZ['start']\n",
        "positive1 = H3F3A_DOHH2_intersect_with_DeepZ[H3F3A_DOHH2_intersect_with_DeepZ['len'] == 10]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwbjE7WIWqA9",
        "outputId": "3816128e-a06f-4fe9-baaf-0f07d58d4995"
      },
      "source": [
        "sz = positive1['len'] // 2\n",
        "positive1['center'] = positive1['start'] + sz\n",
        "positive1['start'] = positive1['center'] - 100\n",
        "positive1['end'] = positive1['center'] + 100\n",
        "\n",
        "positive1 = positive1.drop(columns=['len', 'center'])\n",
        "positive1 = positive1.drop_duplicates()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEEwuhZMXBOw"
      },
      "source": [
        "for i in range(positive1.shape[0]):\n",
        "    start = positive1.iloc[i, :]['start']\n",
        "    end = positive1.iloc[i, :]['end']\n",
        "    name = positive1.iloc[i, :]['chr']\n",
        "    file_name = str(i) + 'z.fa'\n",
        "    !./twoBitToFa http://hgdownload.cse.ucsc.edu/gbdb/hg19/hg19.2bit $file_name -seq=$name -start=$start -end=$end"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBwwy_I9X7OP"
      },
      "source": [
        "!cat *z.fa > pos.fa"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-cjlaRhY-ix",
        "outputId": "44d7dbfd-718c-4435-ef42-bf4708e14284"
      },
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8xuyhzxX-KH"
      },
      "source": [
        "!bedtools subtract -a Genome.bed -b DeepZ.bed > without_deepz.bed"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FvfrFPkZL8Y"
      },
      "source": [
        "!bedtools subtract -a without_deepz.bed -b H3F3A_DOHH2.merge.hg19.bed > negative1.bed"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmIBhJ7NZXzX"
      },
      "source": [
        "negative1 = pd.read_csv('negative1.bed', sep='\\t')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Is2w6O_Zadr"
      },
      "source": [
        "negative1.columns = ['chr', 'start', 'end', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
        "negative1.loc[negative1['end'] - negative1['start'] >= 200, 'end'] = negative1['start'] + 200"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNfTnwZrZdNv"
      },
      "source": [
        "negative1 = negative1.loc[negative1['end'] - negative1['start'] == 200].sample(10000)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNkULo5NZezj"
      },
      "source": [
        "negative1[[\"chr\", \"start\", \"end\", \"4\"]].to_csv('negative.bed', index=None, sep=\"\\t\", header=None)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNvtrk3sZltS",
        "outputId": "b344db2c-2f3b-4223-b1c7-22555845d2b9"
      },
      "source": [
        "#negative1[[\"chr\", \"start\", \"end\", \"8\"]].to_csv('negative.bed', index=None, sep=\"\\t\", header=None)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chr1\t174285404\t174285604\tNM_001366446.1\n",
            "chr18\t46920888\t46921088\tNM_001353211.3\n",
            "chr1\t179712425\t179712625\tNM_173509.3\n",
            "chr6\t36765161\t36765361\tNR_164866.1\n",
            "chr8\t53263939\t53264139\tNM_001352826.2\n",
            "chr8\t130854495\t130854695\tNM_001353251.1\n",
            "chr19\t46218830\t46219030\tNM_001080469.2\n",
            "chr20\t45846756\t45846956\tNM_001281783.3\n",
            "chr2\t25637788\t25637988\tNM_001256308.2\n",
            "chr5\t133451550\t133451750\tNM_201632.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeDSmq5rZt0V",
        "outputId": "b40e8f5c-13a0-46dd-9988-75c22d277ae9"
      },
      "source": [
        "!wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/twoBitToFa\n",
        "!chmod a+x twoBitToFa"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-02 19:29:22--  http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/twoBitToFa\n",
            "Resolving hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)... 128.114.119.163\n",
            "Connecting to hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)|128.114.119.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9367560 (8.9M)\n",
            "Saving to: ‘twoBitToFa.2’\n",
            "\n",
            "twoBitToFa.2        100%[===================>]   8.93M  6.02MB/s    in 1.5s    \n",
            "\n",
            "2021-12-02 19:29:24 (6.02 MB/s) - ‘twoBitToFa.2’ saved [9367560/9367560]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqTg3bR3Zx7O"
      },
      "source": [
        "!./twoBitToFa http://hgdownload.cse.ucsc.edu/gbdb/hg19/hg19.2bit -bed=negative.bed neg.fa\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-S1YuCUn4p0",
        "outputId": "d34eaade-b97c-47f9-a61c-fe7321600ad4"
      },
      "source": [
        "!head neg.fa"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">NM_153348.3\n",
            "CCCCAACTAAAAAGTTCTTATCCTCAGTGCTGTTCATTTTTTCATGTTAT\n",
            "TCGTGAATTAGCAGGCCTCCACTTACGAACCTTTTGAAATGATGCCTGTG\n",
            "TATACTAGAACAACTTGTGACTTACATGCGGTCTTTGTTGGAACGGTCTG\n",
            "CTGCTTGAAGCAGCTTGGGTttcattcatatcccacaaatatttattaag\n",
            ">NM_001354297.2\n",
            "ATTCCTTCAGTTCTTCAAAATCTATTTAGTTGGTAAGTGGATTGTTTAGG\n",
            "Cttaataaattaatataaattacttgaaattcagtaatatttgatatttt\n",
            "aattatgtaatattttaGGGATACAATGTAAATACTCAAGGCTCAAGGAA\n",
            "CTTAGAATAGGGAAAGAATAGTACACAATTGATTTAAAGTATCATGAAAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Oxu5YSanjc"
      },
      "source": [
        "**Обучаем модель**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpIRqCXSalfn"
      },
      "source": [
        "__author__ = 'jasperz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcS-OPlDavpg",
        "outputId": "b288de6b-6486-4d02-838c-ca8838f09cf8"
      },
      "source": [
        "# An object of this class represents a neural network, which you can build, print, train, evaluate, save and load.\n",
        "# Below, the functions are discussed in detail.\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "class NetworkModel:\n",
        "    tf.disable_v2_behavior()\n",
        "\n",
        "    # The constructor function\n",
        "    def __init__(self, file_to_load = None):\n",
        "        tf.reset_default_graph()\n",
        "        self.all_layers = []\n",
        "\n",
        "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\n",
        "        config = tf.ConfigProto(gpu_options=gpu_options)\n",
        "        config.gpu_options.allow_growth = True\n",
        "\n",
        "        self.sess = tf.Session(config=config)\n",
        "\n",
        "        if not file_to_load:\n",
        "            self.X_placeholder = tf.placeholder(tf.float32, [None, 200, 4],name='X_placeholder')\n",
        "            self.Y_placeholder = tf.placeholder(tf.float32, [None, 2],name='Y_placeholder')\n",
        "            self.loaded = False\n",
        "            self.nn = None\n",
        "        else:\n",
        "            self.loaded = True\n",
        "            self._loadNetworkParameters('models/'+file_to_load)\n",
        "            self.X_placeholder = tf.get_default_graph().get_tensor_by_name('X_placeholder:0')\n",
        "            self.Y_placeholder = tf.get_default_graph().get_tensor_by_name('Y_placeholder:0')\n",
        "            self.predictions_softmax = tf.get_default_graph().get_tensor_by_name('softmax_prediction:0')\n",
        "\n",
        "    # Adding the input layer\n",
        "    def addInputLayer(self):\n",
        "        assert len(self.all_layers) == 0, 'The input layer should be the first layer of the network, and can only be added once.'\n",
        "        self.all_layers.append(('Input layer','',self.X_placeholder))\n",
        "\n",
        "    # Adding a convolution layer\n",
        "    def addConvLayer(self, num_of_filters, filter_width, zero_padding = True):\n",
        "        assert len(self.all_layers) > 0 and self.all_layers[0][0].startswith('Input layer')\n",
        "        assert zero_padding in (True,False), 'zero_padding should be True or False (boolean)'\n",
        "        assert 0 < num_of_filters < 500, 'The number of filters specified should be a positive number, smaller than 500'\n",
        "        assert 0 < filter_width < 64, 'The width of your filters should be a positive number, smaller than 64'\n",
        "        assert len(self.all_layers)+1 < 21, 'The total amount of layers should be at most 20'\n",
        "        assert 'Fully-connected layer' not in [typ for typ,_,_ in self.all_layers], 'You cannot add a convolutional layer after a fully-connected layer'\n",
        "        assert 'Softmax (output) layer' not in [typ for typ,_,_ in self.all_layers], 'You cannot add a convolutional layer after a softmax layer'\n",
        "        prev_width = self.all_layers[-1][-1].shape[1]\n",
        "        assert zero_padding or prev_width >= filter_width, 'You cannot add a (non-zeropadded) convolution of width {} when the previous layer has an output width of {}'.format(filter_width,prev_width)\n",
        "        self.all_layers.append(('Convolutional layer',\n",
        "                                '{} filters, width {}, {}zero padding, with ReLU'.format(num_of_filters,\n",
        "                                                                                         filter_width,\n",
        "                                                                                         'no ' if not zero_padding else ''),\n",
        "                                tf.layers.conv1d(self.all_layers[-1][-1],\n",
        "                                                 filters=num_of_filters,\n",
        "                                                 kernel_size=filter_width,\n",
        "                                                 activation=tf.nn.relu,\n",
        "                                                 padding='same' if zero_padding else 'valid')))\n",
        "\n",
        "    # Adding a max pooling layer\n",
        "    def addMaxPoolLayer(self, pool_size):\n",
        "        assert len(self.all_layers) > 0 and self.all_layers[0][0].startswith('Input layer')\n",
        "        assert 'Fully-connected layer' not in [typ for typ,_,_ in self.all_layers], 'You cannot add a pooling layer after a fully-connected layer'\n",
        "        assert 'Softmax (output) layer' not in [typ for typ,_,_ in self.all_layers], 'You cannot add a pooling layer after a softmax layer'\n",
        "        assert len(self.all_layers)+1 < 21, 'The total amount of layers should be at most 20'\n",
        "        assert 0 < pool_size < 50, 'The pool size should be lower than 50'\n",
        "        prev_width = self.all_layers[-1][-1].shape[1]\n",
        "        assert prev_width >= pool_size, 'You cannot add a pooling layer with pool size {} when the previous layer has an output width of {}'.format(pool_size,prev_width)\n",
        "        self.all_layers.append(('Max pooling layer',\n",
        "                                'pool size {}'.format(pool_size),\n",
        "                                tf.layers.max_pooling1d(self.all_layers[-1][-1],\n",
        "                                                        pool_size=pool_size,\n",
        "                                                        strides=pool_size)))\n",
        "\n",
        "    # Adding a fully-connected layer\n",
        "    def addFullyConnectedLayer(self,num_of_neurons):\n",
        "        assert len(self.all_layers) > 0 and self.all_layers[0][0].startswith('Input layer')\n",
        "        assert 'Softmax (output) layer' not in [typ for typ,_,_ in self.all_layers], 'You cannot add a fully-connected layer after a softmax layer'\n",
        "        assert len(self.all_layers)+1 < 21, 'The total amount of layers should be at most 20'\n",
        "        assert 0 < num_of_neurons < 1000, 'The amount of neurons in this layer should be a positive number, lower than 2000'\n",
        "        if len(self.all_layers[-1][-1].shape) > 2:\n",
        "            self.all_layers.append(('Flatten layer',\n",
        "                                    '',\n",
        "                                   tf.layers.flatten(self.all_layers[-1][-1])))\n",
        "        self.all_layers.append(('Fully-connected layer',\n",
        "                                '{} neurons, with ReLU'.format(num_of_neurons),\n",
        "                                tf.layers.dense(self.all_layers[-1][-1],num_of_neurons)))\n",
        "\n",
        "    # Adding an output layer\n",
        "    def addOutputLayer(self):\n",
        "        assert len(self.all_layers) > 0 and self.all_layers[0][0].startswith('Input layer')\n",
        "        assert 'Softmax (output) layer' not in [typ for typ,_,_ in self.all_layers], 'You cannot add a softmax (output) layer after a softmax layer'\n",
        "        assert len(self.all_layers)+1 < 21, 'The total amount of layers should be at most 20'\n",
        "        if len(self.all_layers[-1][-1].shape) > 2:\n",
        "            self.all_layers.append(('Flatten layer',\n",
        "                                    '',\n",
        "                                    tf.contrib.layers.flatten(self.all_layers[-1][-1])))\n",
        "        # assert no output layer yet\n",
        "        # assert # of layers\n",
        "        self.all_layers.append(('Softmax (output) layer',\n",
        "                                '2 neurons',\n",
        "                                tf.layers.dense(self.all_layers[-1][-1], 2,name='logits')))\n",
        "\n",
        "    # Printing out an overview of the layers\n",
        "    def printDetails(self):\n",
        "        print('####################################')\n",
        "        print('Network information:')\n",
        "        # count all parameters:\n",
        "        total_parameters = 0\n",
        "        # iterating over all variables\n",
        "        for variable in tf.trainable_variables():\n",
        "            local_parameters = 1\n",
        "            shape = variable.get_shape()  # getting shape of a variable\n",
        "            for i in shape:\n",
        "                local_parameters *= i.value  # mutiplying dimension values\n",
        "            total_parameters += local_parameters\n",
        "        print('This network has {} trainable parameters.'.format(total_parameters))\n",
        "\n",
        "        for i,(name,info,l) in enumerate(self.all_layers):\n",
        "            try:\n",
        "                print('{: >2d}. {:23} {:50} -> Output size: {}'.format(i, name, info, l.shape))\n",
        "            except AttributeError:\n",
        "                pass\n",
        "        print('')\n",
        "        print('####################################')\n",
        "\n",
        "    # Function to train the network\n",
        "    def train(self, trainX, trainY, validX, validY, n_epochs):\n",
        "        print('####################################')\n",
        "        assert 'Input layer' in [typ for typ,_,_ in self.all_layers], 'You cannot train a model without an input layer'\n",
        "        assert 'Softmax (output) layer' in [typ for typ,_,_ in self.all_layers], 'You cannot train a model without an output layer'\n",
        "        assert self.loaded == False, 'You can not (re)train a model loaded from a file.'\n",
        "        assert 1 < n_epochs < 100, 'The number of epochs should be greater than 1 and lower than 100'\n",
        "        assert all(type(l) == list for l in (trainX, trainY, validX, validY)), 'trainX, trainY, validX and validY should all be lists'\n",
        "        assert all(len(l) > 0 for l in (trainX, trainY, validX, validY)), 'trainX, trainY, validX and validY should not be empty'\n",
        "\n",
        "        assert len(trainX) == len(trainY), 'trainX and trainY should have the same amount of samples'\n",
        "        assert len(trainX[0]) == 200 and len(trainX[0][0]) == 4 and type(trainX[0][0][0]) == int, 'trainX should have size (_, 200, 4) and should contain integers'\n",
        "        assert type(trainY[0]) == int, 'trainY should have length n (for n sequences) and should contain integers'\n",
        "\n",
        "        assert len(validX) == len(validY), 'validX and validY should have the same amount of samples'\n",
        "        assert len(validX[0]) == 200 and len(validX[0][0]) == 4 and type(validX[0][0][0]) == int, 'validX should have size (_, 200, 4) and should contain integers'\n",
        "        assert type(validY[0]) == int, 'validY should have length n (for n sequences) and should contain integers'\n",
        "        # assert input and output layer\n",
        "        self._prepare_training()\n",
        "\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.sess.run(tf.local_variables_initializer())\n",
        "        train_dataset = _Dataset(trainX, trainY)\n",
        "        valid_dataset = _Dataset(validX, validY)\n",
        "        self._printOutputClasses(train_dataset,'training')\n",
        "        self._printOutputClasses(valid_dataset,'validation')\n",
        "\n",
        "        best_valid_score = 999999\n",
        "        print()\n",
        "        print(' {:^5} | {:^14} | {:^14} | {:^11} | {:^11} | {:^8} '.format('epoch','train cost','valid cost','train acc','valid acc','time'))\n",
        "        print('-{:-^6}+{:-^16}+{:-^16}+{:-^13}+{:-^13}+{:-^9}-'.format('','','','','',''))\n",
        "\n",
        "        tr_cost, tr_acc = self._evaluateSet(train_dataset)\n",
        "        va_cost, va_acc = self._evaluateSet(valid_dataset)\n",
        "        print(' {:5d} |   {:2.8f}   |   {:2.8f}   |  {:1.7f}  | {:1.7f}  | {:4.2f}s '.format(0,tr_cost,tr_acc,va_cost,va_acc,0))\n",
        "\n",
        "        for epoch in range(1,n_epochs+1):\n",
        "            epoch_start_time = time.time()\n",
        "            epoch_finished = False\n",
        "            while not epoch_finished:\n",
        "                batch_x, batch_y, epoch_finished = train_dataset.next_batch(256)\n",
        "                self.sess.run(self.train_op, feed_dict={self.X_placeholder: batch_x, self.Y_placeholder: batch_y})\n",
        "            tr_cost, tr_acc = self._evaluateSet(train_dataset)\n",
        "            va_cost, va_acc = self._evaluateSet(valid_dataset)\n",
        "\n",
        "            if va_cost < best_valid_score:\n",
        "                best_valid_score = va_cost\n",
        "                message = '-> model selected'\n",
        "                self._storeNetworkParameters('models/tmp')\n",
        "            else:\n",
        "                message = ''\n",
        "            print(' {:5d} |   {:2.8f}   |   {:2.8f}   |  {:1.7f}  | {:1.7f}  | {:4.2f}s {}'.format(epoch,tr_cost,va_cost,tr_acc,va_acc,time.time()-epoch_start_time,message))\n",
        "\n",
        "        self._loadNetworkParameters('models/tmp')\n",
        "        print('Finished training')\n",
        "        print('####################################')\n",
        "\n",
        "    # Function to generate predictions for a certain dataset.\n",
        "    def generatePredictions(self, testX):\n",
        "        assert len(testX[0]) == 200 and len(testX[0][0]) == 4 and type(testX[0][0][0]) == int, 'testX should have size (_, 200, 4) and should contain integers'\n",
        "        assert self.loaded or 'Input layer' in [typ for typ,_,_ in self.all_layers], 'You cannot test a model without an input layer'\n",
        "        assert self.loaded or 'Softmax (output) layer' in [typ for typ,_,_ in self.all_layers], 'You cannot test a model without an output layer'\n",
        "        # assert input and output layer\n",
        "        all_preds = []\n",
        "        for i in range(math.ceil(len(testX)/256)):\n",
        "            batch_x = np.asarray(testX[i*256:(i+1)*256])\n",
        "            preds = self.sess.run(self.predictions_softmax,feed_dict={self.X_placeholder:batch_x})\n",
        "            for i in range(len(preds)):\n",
        "                all_preds.append((preds[i][0],preds[i][1]))\n",
        "        return all_preds\n",
        "\n",
        "    # Function to save the model\n",
        "    def saveModel(self, file_to_save_to):\n",
        "        assert 'Input layer' in [typ for typ,_,_ in self.all_layers], 'You cannot save a model without an input layer'\n",
        "        assert 'Softmax (output) layer' in [typ for typ,_,_ in self.all_layers], 'You cannot save a model without an output layer'\n",
        "        # assert input and output layer\n",
        "        assert not self.loaded, 'You cannot save a loaded model again.'\n",
        "        self._storeNetworkParameters('models/'+file_to_save_to)\n",
        "\n",
        "    def _prepare_training(self):\n",
        "        # assert all layers -1 == output layer\n",
        "        gs = tf.train.get_or_create_global_step()\n",
        "        self.predictions_softmax = tf.nn.softmax(self.all_layers[-1][-1],name='softmax_prediction')\n",
        "\n",
        "        self.cost_f = tf.losses.softmax_cross_entropy(onehot_labels=self.Y_placeholder, logits=self.all_layers[-1][-1])\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "        self.train_op = self.optimizer.minimize(loss=self.cost_f,global_step=gs)\n",
        "\n",
        "        self.acc_f, self.acc_op = tf.metrics.accuracy(labels=tf.argmax(self.Y_placeholder, axis=1),predictions=tf.argmax(self.predictions_softmax, axis=1),name='metric_acc')\n",
        "        self.metric_var_initializer = tf.variables_initializer(var_list=tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope='metric'))\n",
        "\n",
        "    def _evaluateSet(self, dataset):\n",
        "        self.sess.run(self.metric_var_initializer)\n",
        "        costs = []\n",
        "        batches_done = False\n",
        "        while not batches_done:\n",
        "            batch_x, batch_y, epoch_finished = dataset.next_batch(256)\n",
        "\n",
        "            cost_batch = self.sess.run(self.cost_f, feed_dict={self.X_placeholder: batch_x,self.Y_placeholder: batch_y})\n",
        "            _ = self.sess.run([self.acc_op], feed_dict={self.X_placeholder: batch_x,self.Y_placeholder: batch_y})\n",
        "            costs.extend([cost_batch] * len(batch_y))\n",
        "\n",
        "            if epoch_finished:\n",
        "                batches_done = True\n",
        "\n",
        "        accuracy = self.sess.run([self.acc_f])[0]\n",
        "        return np.average(costs),accuracy\n",
        "\n",
        "    def _printOutputClasses(self, dataset, label):\n",
        "        print()\n",
        "        counts = dataset.getClassCounts()\n",
        "        print('Number of {} examples: {}'.format(label,int(np.sum(counts))))\n",
        "        if len(counts) > 1:\n",
        "            print('Distribution of the {} set:'.format(label))\n",
        "            for i in range(min(10,len(counts))):\n",
        "                print('  # elements of class {} = {}'.format(i,int(counts[i])))\n",
        "\n",
        "    def _storeNetworkParameters(self, saveToDir):\n",
        "        try:\n",
        "            saver = tf.train.Saver()\n",
        "            if not os.path.exists(saveToDir):\n",
        "                os.makedirs(saveToDir)\n",
        "            saver.save(self.sess,saveToDir+'/'+saveToDir[saveToDir.rfind('/')+1:])\n",
        "        except Exception:\n",
        "            print('SOMETHING WENT WRONG WITH STORING SHIT JASPER!! ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "            print(sys.exc_info())\n",
        "            print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "\n",
        "    def _loadNetworkParameters(self, saveToDir):\n",
        "        filename = saveToDir+'/'+saveToDir[saveToDir.rfind('/')+1:]\n",
        "        if self.loaded:\n",
        "            saver = tf.train.import_meta_graph(filename+'.meta')\n",
        "        else:\n",
        "            saver = tf.train.Saver()\n",
        "        saver.restore(self.sess, tf.train.latest_checkpoint(saveToDir))\n",
        "\n",
        "\n",
        "class _Dataset:\n",
        "\n",
        "    def __init__(self,x_data,y_data=None):\n",
        "        if isinstance(x_data,list):\n",
        "            x_data = np.asarray(x_data)\n",
        "\n",
        "        self.index_in_epoch = 0\n",
        "        self.x_data = x_data\n",
        "        self.num_samples = x_data.shape[0]\n",
        "\n",
        "        if y_data:\n",
        "            if isinstance(y_data,list):\n",
        "                y_data = self._convertY(y_data)\n",
        "                self.y_data = y_data\n",
        "        else:\n",
        "            self.y_data = []\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def getClassCounts(self):\n",
        "        return np.sum(self.y_data,axis=0)\n",
        "\n",
        "    def _convertY(self, y_data):\n",
        "        out = np.zeros((len(y_data),2))\n",
        "        for i,cl in enumerate(y_data):\n",
        "            out[i][cl] = 1\n",
        "        return out\n",
        "\n",
        "    def next_batch(self,batch_size):\n",
        "        start = self.index_in_epoch\n",
        "        end = self.index_in_epoch + batch_size\n",
        "\n",
        "        if start == 0:\n",
        "            idx = np.arange(0, self.num_samples)  # get all possible indexes\n",
        "            np.random.shuffle(idx)  # shuffle indexes\n",
        "            self.x_data = self.x_data[idx]\n",
        "            if len(self.y_data) > 0:\n",
        "                self.y_data = self.y_data[idx]\n",
        "\n",
        "        if end < self.num_samples:\n",
        "            self.index_in_epoch = end\n",
        "            return self.x_data[start:end], self.y_data[start:end], False # epoch finished = False\n",
        "        else:\n",
        "            self.index_in_epoch = 0\n",
        "            return self.x_data[start:], self.y_data[start:], True #epoch finished = True\n",
        "\n",
        "\n",
        "    def stepsInEpoch(self,batch_size):\n",
        "        return math.ceil(len(self) / batch_size)\n",
        "\n",
        "    def getX(self):\n",
        "        return self.x_data\n",
        "\n",
        "    def getSequenceLength(self):\n",
        "        return len(self.x_data[0])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSo7_Lema0l6"
      },
      "source": [
        "def readInputs(f1,f2):\n",
        "    lines_pos = open(f1).readlines()\n",
        "    lines_neg = open(f2).readlines()\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for l in convertLines(lines_pos):\n",
        "        X.append(l)\n",
        "        Y.append(1)\n",
        "    for l in convertLines(lines_neg):\n",
        "        X.append(l)\n",
        "        Y.append(0)\n",
        "\n",
        "    return X,Y\n",
        "\n",
        "def convertLines(lines):\n",
        "    v = []\n",
        "    newLines = []\n",
        "    for line in lines:\n",
        "        newline = []\n",
        "        for c in line.strip():\n",
        "            if c == 'A':\n",
        "                v = [1,0,0,0]\n",
        "            elif c == 'C':\n",
        "                v = [0,1,0,0]\n",
        "            elif c == 'G':\n",
        "                v = [0,0,1,0]\n",
        "            elif c == 'T':\n",
        "                v = [0,0,0,1]\n",
        "            newline.append(v)\n",
        "        newLines.append(newline)\n",
        "    return newLines\n",
        "def regular_network():\n",
        "    net_model = NetworkModel()\n",
        "    net_model.addInputLayer()\n",
        "    net_model.addFullyConnectedLayer(50)\n",
        "    net_model.addFullyConnectedLayer(50)\n",
        "    net_model.addOutputLayer()\n",
        "    return net_model"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT7HPdSNtg7Q"
      },
      "source": [
        "def regular_network():\n",
        "    net_model = NetworkModel()\n",
        "    net_model.addInputLayer()\n",
        "    net_model.addFullyConnectedLayer(50)\n",
        "    net_model.addFullyConnectedLayer(50)\n",
        "    net_model.addOutputLayer()\n",
        "    return net_model"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy9JF6Hkth92",
        "outputId": "6130b347-0707-4751-aa20-185176b5b2ee"
      },
      "source": [
        "rm = regular_network()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:523: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:84: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:255: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWF8hkBptnN3"
      },
      "source": [
        "X, y = readInputs('pos.txt', 'neg.txt')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za73ITxrt36D"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhTKWDEQt9bJ"
      },
      "source": [
        "trainX, curX, trainY, curY = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "testX, validX, testY, validY = train_test_split(curX, curY, test_size=0.1, random_state=42)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKQLlm6uuIUi",
        "outputId": "6e006904-3849-46ab-f25f-6d874d42a73e"
      },
      "source": [
        "def conv_network():\n",
        "    con_model = NetworkModel()\n",
        "    con_model.addInputLayer()\n",
        "    con_model.addConvLayer(10, 7)\n",
        "    con_model.addMaxPoolLayer(5)\n",
        "    con_model.addConvLayer(20, 5)\n",
        "    con_model.addMaxPoolLayer(5)\n",
        "    con_model.addFullyConnectedLayer(15)\n",
        "    con_model.addOutputLayer()\n",
        "    return con_model"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIl5aKaXujIQ",
        "outputId": "617be12c-17d7-4952-8dc6-69b7b90d8b5e"
      },
      "source": [
        "My_Model = NetworkModel()\n",
        "My_Model.addInputLayer()\n",
        "My_Model.addConvLayer(10, 7)\n",
        "My_Model.addMaxPoolLayer(5)\n",
        "My_Model.addConvLayer(20, 5)\n",
        "My_Model.addMaxPoolLayer(5)\n",
        "My_Model.addFullyConnectedLayer(15)\n",
        "My_Model.addOutputLayer()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:288: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: UserWarning: `tf.layers.max_pooling1d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling1D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/pooling.py:294: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:523: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:84: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:255: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJfOiLDpuptJ",
        "outputId": "61d682c2-afa4-4905-d095-fe02dd6d3476"
      },
      "source": [
        "My_Model.printDetails()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################################\n",
            "Network information:\n",
            "This network has 3757 trainable parameters.\n",
            " 0. Input layer                                                                -> Output size: (?, 200, 4)\n",
            " 1. Convolutional layer     10 filters, width 7, zero padding, with ReLU       -> Output size: (?, 200, 10)\n",
            " 2. Max pooling layer       pool size 5                                        -> Output size: (?, 40, 10)\n",
            " 3. Convolutional layer     20 filters, width 5, zero padding, with ReLU       -> Output size: (?, 40, 20)\n",
            " 4. Max pooling layer       pool size 5                                        -> Output size: (?, 8, 20)\n",
            " 5. Flatten layer                                                              -> Output size: (?, 160)\n",
            " 6. Fully-connected layer   15 neurons, with ReLU                              -> Output size: (?, 15)\n",
            " 7. Softmax (output) layer  2 neurons                                          -> Output size: (?, 2)\n",
            "\n",
            "####################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAtnierguxB-",
        "outputId": "785b807a-bd3a-4e55-e04e-91b603a32e15"
      },
      "source": [
        "My_Model.train(trainX, trainY, validX, validY, 15)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################################\n",
            "\n",
            "Number of training examples: 7005\n",
            "Distribution of the training set:\n",
            "  # elements of class 0 = 6999\n",
            "  # elements of class 1 = 6\n",
            "\n",
            "Number of validation examples: 301\n",
            "Distribution of the validation set:\n",
            "  # elements of class 0 = 301\n",
            "  # elements of class 1 = 0\n",
            "\n",
            " epoch |   train cost   |   valid cost   |  train acc  |  valid acc  |   time   \n",
            "-------+----------------+----------------+-------------+-------------+----------\n",
            "     0 |   0.30837816   |   0.99857247   |  0.3103329  | 1.0000000  | 0.00s \n",
            "     1 |   0.00758863   |   0.00013629   |  0.9991435  | 1.0000000  | 1.76s -> model selected\n",
            "     2 |   0.00738818   |   0.00016736   |  0.9991435  | 1.0000000  | 1.47s \n",
            "     3 |   0.00691130   |   0.00033344   |  0.9991435  | 1.0000000  | 1.52s \n",
            "     4 |   0.00661097   |   0.00062330   |  0.9991435  | 1.0000000  | 1.45s \n",
            "     5 |   0.00650680   |   0.00090470   |  0.9991435  | 1.0000000  | 1.51s \n",
            "     6 |   0.00645361   |   0.00080487   |  0.9991435  | 1.0000000  | 1.43s \n",
            "     7 |   0.00638795   |   0.00088894   |  0.9991435  | 1.0000000  | 1.48s \n",
            "     8 |   0.00632150   |   0.00089499   |  0.9991435  | 1.0000000  | 1.44s \n",
            "     9 |   0.00627030   |   0.00078778   |  0.9991435  | 1.0000000  | 1.47s \n",
            "    10 |   0.00620311   |   0.00094309   |  0.9991435  | 1.0000000  | 1.46s \n",
            "    11 |   0.00613311   |   0.00091854   |  0.9991435  | 1.0000000  | 1.47s \n",
            "    12 |   0.00606903   |   0.00078467   |  0.9991435  | 1.0000000  | 1.46s \n",
            "    13 |   0.00601518   |   0.00077555   |  0.9991435  | 1.0000000  | 1.45s \n",
            "    14 |   0.00594480   |   0.00092340   |  0.9991435  | 1.0000000  | 1.40s \n",
            "    15 |   0.00587809   |   0.00089160   |  0.9991435  | 1.0000000  | 1.46s \n",
            "INFO:tensorflow:Restoring parameters from models/tmp/tmp\n",
            "Finished training\n",
            "####################################\n"
          ]
        }
      ]
    }
  ]
}